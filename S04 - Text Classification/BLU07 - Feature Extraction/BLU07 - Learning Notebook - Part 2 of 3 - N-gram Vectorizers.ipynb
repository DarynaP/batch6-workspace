{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "from numpy import inf\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at some Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning all about tokenization and regexes, let's start doing some cool stuff and apply it to a true dataset!\n",
    "\n",
    "In Part II of this BLU, we're going to look into how to transform text into something that is meaningful to a machine. As you may have noticed, text data is a bit different from other datasets you might have seen -- it's just a bunch of words strung together! Where are the features in a tightly organized table of examples? Unlike other data you might have worked with in previous BLUs, text is unstructured and thus needs some additional work on our end to make it structured and ready to be handled by a machine learning algorithm.\n",
    "\n",
    "Language can be messy, but one thing is clear: we need features. To get features from a string (text or a **document**), one way is to **vectorize** it. Normally, this means that our feature space is the **vocabulary** of the examples present in our dataset. That is, the set of unique words we can find in all of the training examples.\n",
    "\n",
    "<img src=\"./media/vectors.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "But enough talk - let's get our hands dirty!\n",
    "\n",
    "In this BLU, we're going to work with some movie reviews from IMDB. Let's load the dataset into pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Aldolpho (Steve Buscemi), an aspiring film mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>An unfunny, unworthy picture which is an undes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>A failure. The movie was just not good. It has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw this movie Sunday afternoon. I absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Disney goes to the well one too many times as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0  Negative  Aldolpho (Steve Buscemi), an aspiring film mak...\n",
       "1  Negative  An unfunny, unworthy picture which is an undes...\n",
       "2  Negative  A failure. The movie was just not good. It has...\n",
       "3  Positive  I saw this movie Sunday afternoon. I absolutel...\n",
       "4  Negative  Disney goes to the well one too many times as ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two columns in this dataset - one for the labels and another for the text of the movie review. Each example is labeled as a positive or negative review. Our goal is to retrieve meaningful features from the text so a machine can predict if a given unlabeled review is positive or negative.\n",
    "\n",
    "Let's see a positive and a negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "\"The Lion King\" is without a doubt my favorite Disney movie of all time, so I figured maybe I should give the sequels a chance and I did. Lion King 1 1/2 was pretty good and had it's good laughs and fun with Timon and Pumba. Only problem, I feel sometimes no explanations are needed because they can create plot holes and just the feeling of wanting your own explanation. Well, I would highly recommend this movie for lion King fans or just a night with the family. It's a fun flick with the same laughs and lovable characters as the first. So, hopefully, I'll get the same with the third installment to the Lion King series. Sit back and just think Hakuna Matata! It means no worries! <br /><br />8/10\n"
     ]
    }
   ],
   "source": [
    "pos_example = df.text[4835]\n",
    "print(df.sentiment[4835])\n",
    "print(pos_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! So that is a review about *The Lion King 1 1/2* (a.k.a. *The Lion King 3* in some countries). It seems the reviewer liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Disney goes to the well one too many times as anybody who has seen the original LITTLE MERMAID will feel blatantly ripped off. Celebrating the birth of their daughter Melody, Ariel and Eric plan on introducing her to King Triton. The celebration is quickly crashed by Ursula 's sister, Morgana who plans to use Melody as a defense tool to get the King 's trident. Stopping the attack, Ariel and Eric build a wall around the ocean while Melody grows up wondering why she cannot go in there.<br /><br />Awful and terrible is what describes this direct to video sequel. LITTLE MERMAID 2 gives you that feeling everything you watch seemed to have come straight other Disney movies. I guess Disney can only plagiarize itself! Do not tell me that the penguin and walrus does not remind you of another duo from the LION KING!<br /><br />Other disappointing moments include the rematch between Sebastien and Louie, the royal chef. They terribly under played it! The climax between Morgana and EVERYONE seemed to be another disappointment.<br /><br />I will not give anything away, but in 75 minutes, everything seemed incredibly cramped and too much to handle. An embarrassment to Disney, LITTLE MERMAID 2 is better left to rent and laugh at. Then you can prepare for the rest of the other sequels Disney is going to drown you in later on.\n"
     ]
    }
   ],
   "source": [
    "neg_example = df.text[4]\n",
    "print(df.sentiment[4])\n",
    "print(neg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. I guess that's a pass for this one, right?\n",
    "\n",
    "Let's get the first 200 documents of this dataset to run experiments faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've learned in Learning Notebook - Part 1, we can tokenize and stem our text to extract better features. Let's initialize our favorite tokenizer and stemmer. For now, we choose to keep stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in Part 1, applying `ignore_stopwords` to this function prevents the stemming of stopwords, if they are present in the sentence.\n",
    "\n",
    "We can also use a regex to clean our sentences. We can see from the examples above that our corpus has some HTML substrings `<br />` that are only polluting the sentences. We can remove them with `re.sub()` by substituting every substring that matches the regex `<[^>]*>` with an empty string.\n",
    "\n",
    "We will define a `preprocess()` method that removes these unnecessary HTML tags, tokenizes, and stems our corpus's sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove html tags\n",
    "    doc = re.sub(\"<[^>]*>\", \"\", doc)\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # remove punctuation\n",
    "    # string.punctuation is a utility that allows us to not have to define all punctuation characters by ourselves\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    # stem\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    new_doc = \" \".join(stems)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### A small note on punctuation removal\n",
    "\n",
    "Note that above, we've used `string.punctuation` instead of defining a list of characters. This is a handy constant provided by the `string` package that we can use instead of defining our own punctuation string. \n",
    "\n",
    "You can see below the characters included:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't cover everything (remember the quotation marks from the previous unit) but it is still a nice utility for us to know about. \n",
    "\n",
    "In the example above we're using it in a list comprehension to extract single punctuation characters from our list of words returned by the tokenizer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', 'Yes', 'it', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? Yes, it is a test.\"\n",
    "\n",
    "print([word for word in tokenizer.tokenize(text) if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's easy to find examples where this is suboptimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', 'No', 'it', 'isn', 't', '...']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? No, it isn't ...\"\n",
    "\n",
    "print([word for word in tokenizer.tokenize(text) if word not in string.punctuation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `...` wasn't considered as punctuation under this rule. One approach that you can take instead is to completely clean the sentence of punctuation by using regex combined with this utility:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a test  No  it isn  t \n",
      "Is this a test ? No , it isn ' t ...\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? No, it isn't ...\"\n",
    "\n",
    "pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "\n",
    "sentence = \" \".join(tokenizer.tokenize(text))\n",
    "\n",
    "print(re.sub(pattern, '', sentence))\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what is your goal, you may opt for the simpler method or the more complex one.\n",
    "\n",
    "---\n",
    "\n",
    "But back to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one of the above examples again, after we cleaned the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disney goe to the well one too mani time as anybodi who has seen the origin littl mermaid will feel blatant rip off celebr the birth of their daughter melodi ariel and eric plan on introduc her to king triton the celebr is quick crash by ursula s sister morgana who plan to use melodi as a defens tool to get the king s trident stop the attack ariel and eric build a wall around the ocean while melodi grow up wonder why she cannot go in there aw and terribl is what describ this direct to video sequel littl mermaid 2 give you that feel everyth you watch seem to have come straight other disney movi i guess disney can only plagiar itself do not tell me that the penguin and walrus does not remind you of anoth duo from the lion king other disappoint moment includ the rematch between sebastien and louie the royal chef they terribl under play it the climax between morgana and everyon seem to be anoth disappoint i will not give anyth away but in 75 minut everyth seem incred cramp and too much to handl an embarrass to disney littl mermaid 2 is better left to rent and laugh at then you can prepar for the rest of the other sequel disney is go to drown you in later on'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we may not understand it as well now, but we actually just made the text much easier for a machine to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, our feature space in text will be the vocabulary of our data. In our example, this is the set of unique words and symbols present in our documents.\n",
    "\n",
    "To create our vocabulary, we will use a `Counter()`. `Counter()` is a dictionary that counts the number of occurrences of different tokens in a list and can be updated with each sentence of our corpus.\n",
    "\n",
    "After getting all counts for each unique token, we sort our dictionary by counts using `Counter()`'s built-in method `.most_common()`, and store everything in an `OrderedDict()`. This makes sure our vectorized representations of the documents will be ordered according to the most common words in the whole corpus (this is not required, but makes data visualization much nicer!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('the', 2706),\n",
       "             ('a', 1361),\n",
       "             ('and', 1349),\n",
       "             ('of', 1205),\n",
       "             ('to', 1115),\n",
       "             ('is', 815),\n",
       "             ('it', 786),\n",
       "             ('in', 719),\n",
       "             ('i', 690),\n",
       "             ('this', 594),\n",
       "             ('that', 581),\n",
       "             ('s', 541),\n",
       "             ('movi', 459),\n",
       "             ('film', 400),\n",
       "             ('as', 377),\n",
       "             ('but', 358),\n",
       "             ('with', 357),\n",
       "             ('for', 315),\n",
       "             ('was', 305),\n",
       "             ('t', 295),\n",
       "             ('on', 273),\n",
       "             ('are', 259),\n",
       "             ('you', 259),\n",
       "             ('not', 247),\n",
       "             ('one', 238),\n",
       "             ('be', 225),\n",
       "             ('they', 220),\n",
       "             ('he', 215),\n",
       "             ('his', 214),\n",
       "             ('at', 211),\n",
       "             ('have', 211),\n",
       "             ('like', 204),\n",
       "             ('all', 197),\n",
       "             ('by', 187),\n",
       "             ('from', 186),\n",
       "             ('has', 173),\n",
       "             ('an', 171),\n",
       "             ('who', 171),\n",
       "             ('or', 155),\n",
       "             ('there', 150),\n",
       "             ('about', 149),\n",
       "             ('so', 146),\n",
       "             ('just', 139),\n",
       "             ('if', 138),\n",
       "             ('more', 133),\n",
       "             ('what', 132),\n",
       "             ('her', 130),\n",
       "             ('time', 124),\n",
       "             ('out', 121),\n",
       "             ('get', 118),\n",
       "             ('very', 118),\n",
       "             ('can', 116),\n",
       "             ('up', 115),\n",
       "             ('some', 112),\n",
       "             ('their', 111),\n",
       "             ('watch', 110),\n",
       "             ('when', 109),\n",
       "             ('other', 107),\n",
       "             ('make', 106),\n",
       "             ('even', 105),\n",
       "             ('see', 105),\n",
       "             ('my', 101),\n",
       "             ('good', 99),\n",
       "             ('stori', 99),\n",
       "             ('charact', 98),\n",
       "             ('end', 97),\n",
       "             ('scene', 96),\n",
       "             ('no', 95),\n",
       "             ('would', 94),\n",
       "             ('she', 94),\n",
       "             ('well', 92),\n",
       "             ('much', 92),\n",
       "             ('than', 92),\n",
       "             ('peopl', 92),\n",
       "             ('me', 91),\n",
       "             ('were', 90),\n",
       "             ('which', 88),\n",
       "             ('had', 88),\n",
       "             ('only', 87),\n",
       "             ('love', 86),\n",
       "             ('most', 86),\n",
       "             ('think', 83),\n",
       "             ('we', 82),\n",
       "             ('first', 81),\n",
       "             ('made', 79),\n",
       "             ('will', 79),\n",
       "             ('great', 78),\n",
       "             ('been', 77),\n",
       "             ('seem', 77),\n",
       "             ('thing', 75),\n",
       "             ('realli', 75),\n",
       "             ('its', 74),\n",
       "             ('because', 73),\n",
       "             ('look', 72),\n",
       "             ('too', 71),\n",
       "             ('after', 71),\n",
       "             ('bad', 70),\n",
       "             ('do', 70),\n",
       "             ('could', 68),\n",
       "             ('two', 68),\n",
       "             ('show', 68),\n",
       "             ('him', 68),\n",
       "             ('how', 68),\n",
       "             ('them', 67),\n",
       "             ('go', 66),\n",
       "             ('don', 66),\n",
       "             ('way', 66),\n",
       "             ('best', 66),\n",
       "             ('act', 66),\n",
       "             ('better', 64),\n",
       "             ('play', 64),\n",
       "             ('know', 64),\n",
       "             ('where', 63),\n",
       "             ('say', 63),\n",
       "             ('year', 62),\n",
       "             ('come', 62),\n",
       "             ('any', 62),\n",
       "             ('did', 61),\n",
       "             ('into', 61),\n",
       "             ('over', 60),\n",
       "             ('such', 59),\n",
       "             ('tri', 59),\n",
       "             (').', 58),\n",
       "             ('off', 57),\n",
       "             ('being', 57),\n",
       "             ('work', 56),\n",
       "             ('lot', 56),\n",
       "             ('--', 55),\n",
       "             ('want', 54),\n",
       "             ('seen', 54),\n",
       "             ('littl', 54),\n",
       "             ('then', 54),\n",
       "             ('mani', 53),\n",
       "             ('while', 51),\n",
       "             ('never', 51),\n",
       "             ('doesn', 51),\n",
       "             ('part', 50),\n",
       "             ('does', 49),\n",
       "             ('these', 49),\n",
       "             ('...', 49),\n",
       "             ('),', 48),\n",
       "             ('still', 48),\n",
       "             ('director', 48),\n",
       "             ('someth', 48),\n",
       "             ('find', 48),\n",
       "             ('why', 47),\n",
       "             ('actual', 47),\n",
       "             ('actor', 47),\n",
       "             ('start', 47),\n",
       "             ('plot', 46),\n",
       "             ('interest', 46),\n",
       "             ('ever', 45),\n",
       "             ('take', 44),\n",
       "             ('your', 44),\n",
       "             ('use', 44),\n",
       "             ('give', 44),\n",
       "             ('should', 43),\n",
       "             ('life', 43),\n",
       "             ('also', 43),\n",
       "             ('origin', 42),\n",
       "             ('10', 42),\n",
       "             ('ve', 42),\n",
       "             ('funni', 42),\n",
       "             ('perform', 42),\n",
       "             ('live', 41),\n",
       "             ('though', 41),\n",
       "             ('m', 41),\n",
       "             ('few', 40),\n",
       "             ('friend', 40),\n",
       "             ('through', 39),\n",
       "             ('new', 39),\n",
       "             ('man', 38),\n",
       "             ('bit', 38),\n",
       "             ('back', 37),\n",
       "             ('cast', 37),\n",
       "             ('point', 37),\n",
       "             ('horror', 37),\n",
       "             ('old', 35),\n",
       "             ('anoth', 35),\n",
       "             ('re', 35),\n",
       "             ('fact', 35),\n",
       "             ('each', 35),\n",
       "             ('high', 35),\n",
       "             ('real', 34),\n",
       "             ('feel', 34),\n",
       "             ('here', 34),\n",
       "             ('guy', 34),\n",
       "             ('same', 34),\n",
       "             ('enough', 34),\n",
       "             ('name', 33),\n",
       "             ('star', 33),\n",
       "             ('without', 33),\n",
       "             ('turn', 33),\n",
       "             ('alway', 33),\n",
       "             ('least', 33),\n",
       "             ('long', 33),\n",
       "             ('help', 32),\n",
       "             ('may', 32),\n",
       "             ('before', 32),\n",
       "             ('day', 32),\n",
       "             ('differ', 32),\n",
       "             ('direct', 32),\n",
       "             ('fan', 32),\n",
       "             ('probabl', 32),\n",
       "             ('now', 32),\n",
       "             ('mayb', 32),\n",
       "             ('got', 31),\n",
       "             ('music', 31),\n",
       "             ('minut', 31),\n",
       "             ('girl', 31),\n",
       "             ('problem', 31),\n",
       "             ('american', 31),\n",
       "             ('becom', 31),\n",
       "             ('young', 31),\n",
       "             ('wonder', 30),\n",
       "             ('didn', 30),\n",
       "             ('although', 30),\n",
       "             ('person', 30),\n",
       "             ('.\"', 30),\n",
       "             ('set', 30),\n",
       "             ('isn', 30),\n",
       "             ('beauti', 29),\n",
       "             ('almost', 29),\n",
       "             ('those', 29),\n",
       "             ('kill', 29),\n",
       "             ('enjoy', 29),\n",
       "             ('big', 29),\n",
       "             ('noth', 28),\n",
       "             ('laugh', 28),\n",
       "             ('around', 28),\n",
       "             ('last', 28),\n",
       "             ('quit', 28),\n",
       "             ('\".', 28),\n",
       "             ('run', 28),\n",
       "             ('reason', 28),\n",
       "             ('half', 28),\n",
       "             ('between', 27),\n",
       "             ('rather', 27),\n",
       "             ('hard', 27),\n",
       "             ('comedi', 27),\n",
       "             ('again', 27),\n",
       "             ('wood', 27),\n",
       "             ('might', 27),\n",
       "             ('sure', 26),\n",
       "             ('fun', 26),\n",
       "             ('everyth', 26),\n",
       "             ('hope', 26),\n",
       "             ('d', 26),\n",
       "             ('shot', 26),\n",
       "             ('kid', 26),\n",
       "             ('demon', 26),\n",
       "             ('everi', 26),\n",
       "             ('\\x96', 26),\n",
       "             ('second', 25),\n",
       "             ('howev', 25),\n",
       "             ('action', 25),\n",
       "             ('own', 25),\n",
       "             ('our', 25),\n",
       "             ('right', 25),\n",
       "             ('sens', 25),\n",
       "             ('\",', 25),\n",
       "             ('seri', 25),\n",
       "             ('keep', 25),\n",
       "             ('need', 25),\n",
       "             ('until', 24),\n",
       "             ('poor', 24),\n",
       "             ('mean', 24),\n",
       "             ('especi', 24),\n",
       "             ('moment', 24),\n",
       "             ('effect', 24),\n",
       "             ('worst', 24),\n",
       "             ('camera', 24),\n",
       "             ('both', 24),\n",
       "             ('audienc', 24),\n",
       "             ('move', 24),\n",
       "             ('believ', 24),\n",
       "             ('father', 24),\n",
       "             ('anim', 24),\n",
       "             ('idea', 23),\n",
       "             ('boy', 23),\n",
       "             ('during', 23),\n",
       "             ('2', 23),\n",
       "             ('later', 23),\n",
       "             ('am', 23),\n",
       "             ('happen', 23),\n",
       "             ('obvious', 23),\n",
       "             ('yes', 23),\n",
       "             ('kind', 23),\n",
       "             ('recommend', 23),\n",
       "             ('featur', 23),\n",
       "             ('hour', 23),\n",
       "             ('documentari', 23),\n",
       "             ('thought', 23),\n",
       "             ('sinc', 23),\n",
       "             ('complet', 22),\n",
       "             ('money', 22),\n",
       "             ('tell', 22),\n",
       "             ('away', 22),\n",
       "             ('put', 22),\n",
       "             ('special', 22),\n",
       "             ('main', 22),\n",
       "             ('place', 22),\n",
       "             ('yet', 22),\n",
       "             ('home', 22),\n",
       "             ('sound', 22),\n",
       "             ('anyon', 22),\n",
       "             ('pretti', 22),\n",
       "             ('dvd', 22),\n",
       "             ('won', 22),\n",
       "             ('final', 21),\n",
       "             ('goe', 21),\n",
       "             ('die', 21),\n",
       "             ('us', 21),\n",
       "             ('produc', 21),\n",
       "             ('view', 21),\n",
       "             ('read', 21),\n",
       "             ('usual', 21),\n",
       "             ('lost', 21),\n",
       "             ('begin', 21),\n",
       "             ('favorit', 21),\n",
       "             ('entertain', 21),\n",
       "             ('review', 21),\n",
       "             ('sort', 21),\n",
       "             ('viewer', 20),\n",
       "             ('script', 20),\n",
       "             ('rate', 20),\n",
       "             ('hand', 20),\n",
       "             ('disappoint', 20),\n",
       "             ('everyon', 20),\n",
       "             ('incred', 20),\n",
       "             ('black', 20),\n",
       "             ('stand', 20),\n",
       "             ('perhap', 20),\n",
       "             ('ask', 20),\n",
       "             ('famili', 20),\n",
       "             ('qualiti', 20),\n",
       "             ('war', 20),\n",
       "             ('role', 20),\n",
       "             ('bruce', 20),\n",
       "             ('saw', 19),\n",
       "             ('video', 19),\n",
       "             ('guess', 19),\n",
       "             ('left', 19),\n",
       "             ('true', 19),\n",
       "             ('understand', 19),\n",
       "             ('els', 19),\n",
       "             ('write', 19),\n",
       "             ('mention', 19),\n",
       "             ('whole', 19),\n",
       "             ('appear', 19),\n",
       "             ('someon', 19),\n",
       "             ('evil', 19),\n",
       "             ('call', 19),\n",
       "             ('ll', 19),\n",
       "             ('song', 18),\n",
       "             ('decid', 18),\n",
       "             ('world', 18),\n",
       "             ('chang', 18),\n",
       "             ('down', 18),\n",
       "             ('titl', 18),\n",
       "             ('episod', 18),\n",
       "             ('word', 18),\n",
       "             ('mind', 18),\n",
       "             ('power', 18),\n",
       "             ('except', 18),\n",
       "             ('meet', 18),\n",
       "             ('deserv', 18),\n",
       "             ('coupl', 18),\n",
       "             ('given', 18),\n",
       "             ('care', 18),\n",
       "             ('couldn', 18),\n",
       "             ('nice', 18),\n",
       "             ('short', 18),\n",
       "             ('sad', 18),\n",
       "             ('hit', 18),\n",
       "             ('releas', 18),\n",
       "             ('worth', 17),\n",
       "             ('terribl', 17),\n",
       "             ('either', 17),\n",
       "             ('fight', 17),\n",
       "             ('head', 17),\n",
       "             ('perfect', 17),\n",
       "             ('found', 17),\n",
       "             ('heart', 17),\n",
       "             ('human', 17),\n",
       "             ('emot', 17),\n",
       "             ('school', 17),\n",
       "             ('light', 17),\n",
       "             ('miss', 17),\n",
       "             ('john', 17),\n",
       "             ('togeth', 17),\n",
       "             ('himself', 17),\n",
       "             ('having', 17),\n",
       "             ('particular', 17),\n",
       "             ('classic', 17),\n",
       "             ('leav', 17),\n",
       "             ('hollywood', 17),\n",
       "             ('screen', 17),\n",
       "             ('let', 17),\n",
       "             ('three', 17),\n",
       "             ('festiv', 17),\n",
       "             ('hous', 17),\n",
       "             ('must', 17),\n",
       "             ('simpli', 17),\n",
       "             ('far', 17),\n",
       "             ('aw', 16),\n",
       "             ('line', 16),\n",
       "             ('disney', 16),\n",
       "             ('absolut', 16),\n",
       "             ('sequel', 16),\n",
       "             ('rest', 16),\n",
       "             ('forc', 16),\n",
       "             ('white', 16),\n",
       "             ('said', 16),\n",
       "             ('eye', 16),\n",
       "             ('suppos', 16),\n",
       "             ('once', 16),\n",
       "             ('rememb', 16),\n",
       "             ('situat', 16),\n",
       "             ('against', 16),\n",
       "             ('biko', 16),\n",
       "             ('follow', 16),\n",
       "             ('stupid', 16),\n",
       "             ('wish', 16),\n",
       "             ('face', 16),\n",
       "             ('pull', 16),\n",
       "             ('cours', 16),\n",
       "             ('unfortun', 16),\n",
       "             ('wife', 16),\n",
       "             ('expect', 16),\n",
       "             ('lili', 16),\n",
       "             ('steve', 15),\n",
       "             ('product', 15),\n",
       "             ('chanc', 15),\n",
       "             ('pictur', 15),\n",
       "             ('done', 15),\n",
       "             ('1', 15),\n",
       "             ('serious', 15),\n",
       "             ('stop', 15),\n",
       "             ('mr', 15),\n",
       "             ('sever', 15),\n",
       "             ('style', 15),\n",
       "             ('comment', 15),\n",
       "             ('less', 15),\n",
       "             ('definit', 15),\n",
       "             ('men', 15),\n",
       "             ('age', 15),\n",
       "             ('touch', 15),\n",
       "             ('tv', 15),\n",
       "             (',\"', 15),\n",
       "             ('budget', 15),\n",
       "             ('son', 15),\n",
       "             ('hilari', 15),\n",
       "             ('michael', 15),\n",
       "             ('danni', 15),\n",
       "             ('includ', 14),\n",
       "             ('anyth', 14),\n",
       "             ('wast', 14),\n",
       "             ('portray', 14),\n",
       "             ('myself', 14),\n",
       "             ('version', 14),\n",
       "             ('natur', 14),\n",
       "             ('stuff', 14),\n",
       "             ('doing', 14),\n",
       "             ('went', 14),\n",
       "             ('lead', 14),\n",
       "             ('georg', 14),\n",
       "             ('creat', 14),\n",
       "             ('extrem', 14),\n",
       "             ('class', 14),\n",
       "             ('instead', 14),\n",
       "             ('talk', 14),\n",
       "             ('excel', 14),\n",
       "             ('alien', 14),\n",
       "             ('entir', 14),\n",
       "             ('dialogu', 14),\n",
       "             ('b', 14),\n",
       "             ('piec', 14),\n",
       "             ('futur', 14),\n",
       "             ('rich', 14),\n",
       "             ('scari', 14),\n",
       "             ('wasn', 14),\n",
       "             ('bear', 14),\n",
       "             ('strang', 13),\n",
       "             ('deal', 13),\n",
       "             ('materi', 13),\n",
       "             ('actress', 13),\n",
       "             ('daughter', 13),\n",
       "             ('excit', 13),\n",
       "             ('came', 13),\n",
       "             ('next', 13),\n",
       "             ('voic', 13),\n",
       "             ('etc', 13),\n",
       "             ('predict', 13),\n",
       "             ('speak', 13),\n",
       "             ('lack', 13),\n",
       "             ('drama', 13),\n",
       "             ('effort', 13),\n",
       "             ('fall', 13),\n",
       "             ('involv', 13),\n",
       "             ('surpris', 13),\n",
       "             ('today', 13),\n",
       "             ('plenti', 13),\n",
       "             ('south', 13),\n",
       "             ('certain', 13),\n",
       "             ('cinema', 13),\n",
       "             ('spirit', 13),\n",
       "             ('clear', 13),\n",
       "             ('ed', 13),\n",
       "             ('confus', 13),\n",
       "             ('small', 13),\n",
       "             ('success', 13),\n",
       "             ('lame', 13),\n",
       "             ('top', 13),\n",
       "             ('stay', 13),\n",
       "             ('bore', 13),\n",
       "             ('zombi', 13),\n",
       "             ('sex', 13),\n",
       "             ('horribl', 13),\n",
       "             ('brilliant', 13),\n",
       "             ('utter', 13),\n",
       "             ('woman', 13),\n",
       "             ('town', 13),\n",
       "             ('accept', 13),\n",
       "             ('marri', 12),\n",
       "             ('valu', 12),\n",
       "             ('written', 12),\n",
       "             ('plan', 12),\n",
       "             ('itself', 12),\n",
       "             ('thank', 12),\n",
       "             ('general', 12),\n",
       "             ('edit', 12),\n",
       "             ('base', 12),\n",
       "             ('manag', 12),\n",
       "             ('four', 12),\n",
       "             ('admit', 12),\n",
       "             ('countri', 12),\n",
       "             ('messag', 12),\n",
       "             ('case', 12),\n",
       "             ('score', 12),\n",
       "             ('night', 12),\n",
       "             ('death', 12),\n",
       "             ('constant', 12),\n",
       "             ('spoiler', 12),\n",
       "             ('avoid', 12),\n",
       "             ('strong', 12),\n",
       "             ('car', 12),\n",
       "             ('art', 12),\n",
       "             ('modern', 12),\n",
       "             ('often', 12),\n",
       "             ('group', 12),\n",
       "             ('job', 12),\n",
       "             ('fine', 12),\n",
       "             ('offic', 12),\n",
       "             ('sit', 12),\n",
       "             ('charm', 12),\n",
       "             ('total', 12),\n",
       "             ('inmat', 12),\n",
       "             ('adventur', 11),\n",
       "             ('opinion', 11),\n",
       "             ('soundtrack', 11),\n",
       "             ('parent', 11),\n",
       "             ('truli', 11),\n",
       "             ('felt', 11),\n",
       "             ('buy', 11),\n",
       "             ('throughout', 11),\n",
       "             ('god', 11),\n",
       "             ('open', 11),\n",
       "             ('pass', 11),\n",
       "             ('upon', 11),\n",
       "             ('event', 11),\n",
       "             ('.)', 11),\n",
       "             ('question', 11),\n",
       "             ('consid', 11),\n",
       "             ('anyway', 11),\n",
       "             ('clever', 11),\n",
       "             ('narrat', 11),\n",
       "             ('toward', 11),\n",
       "             ('wors', 11),\n",
       "             ('element', 11),\n",
       "             ('theme', 11),\n",
       "             ('writer', 11),\n",
       "             ('japanes', 11),\n",
       "             ('add', 11),\n",
       "             ('relat', 11),\n",
       "             ('late', 11),\n",
       "             ('memor', 11),\n",
       "             ('fiction', 11),\n",
       "             ('filmmak', 11),\n",
       "             ('matter', 11),\n",
       "             ('amaz', 11),\n",
       "             ('book', 11),\n",
       "             ('exampl', 11),\n",
       "             ('save', 11),\n",
       "             ('abl', 11),\n",
       "             ('achiev', 11),\n",
       "             ('major', 11),\n",
       "             ('deliv', 11),\n",
       "             ('scienc', 11),\n",
       "             ('basic', 11),\n",
       "             ('fail', 11),\n",
       "             ('shown', 11),\n",
       "             ('box', 11),\n",
       "             ('near', 11),\n",
       "             ('land', 11),\n",
       "             ('suspens', 11),\n",
       "             ('footag', 11),\n",
       "             ('pleas', 11),\n",
       "             ('dead', 11),\n",
       "             ('low', 11),\n",
       "             ('genr', 11),\n",
       "             ('dull', 11),\n",
       "             ('compel', 11),\n",
       "             ('fli', 11),\n",
       "             ('court', 11),\n",
       "             ('mother', 10),\n",
       "             ('joe', 10),\n",
       "             ('likabl', 10),\n",
       "             ('king', 10),\n",
       "             ('lee', 10),\n",
       "             ('self', 10),\n",
       "             ('themselves', 10),\n",
       "             ('harri', 10),\n",
       "             ('finish', 10),\n",
       "             ('possibl', 10),\n",
       "             ('singl', 10),\n",
       "             ('joke', 10),\n",
       "             ('dumb', 10),\n",
       "             ('full', 10),\n",
       "             ('figur', 10),\n",
       "             ('present', 10),\n",
       "             ('escap', 10),\n",
       "             ('cute', 10),\n",
       "             ('histori', 10),\n",
       "             ('remak', 10),\n",
       "             ('behind', 10),\n",
       "             ('whose', 10),\n",
       "             ('among', 10),\n",
       "             ('bring', 10),\n",
       "             ('credit', 10),\n",
       "             ('interview', 10),\n",
       "             ('robert', 10),\n",
       "             ('citi', 10),\n",
       "             ('o', 10),\n",
       "             ('violenc', 10),\n",
       "             ('wrong', 10),\n",
       "             ('inspir', 10),\n",
       "             ('appar', 10),\n",
       "             ('hero', 10),\n",
       "             ('allow', 10),\n",
       "             ('ad', 10),\n",
       "             ('magic', 10),\n",
       "             ('result', 10),\n",
       "             ('unlik', 10),\n",
       "             ('80', 10),\n",
       "             ('due', 10),\n",
       "             ('wait', 10),\n",
       "             ('oh', 10),\n",
       "             ('shock', 10),\n",
       "             ('children', 10),\n",
       "             ('jane', 10),\n",
       "             ('silli', 10),\n",
       "             ('russian', 10),\n",
       "             ('f', 10),\n",
       "             ('took', 10),\n",
       "             ('immedi', 10),\n",
       "             ('prison', 10),\n",
       "             ('hagar', 10),\n",
       "             ('apart', 9),\n",
       "             ('along', 9),\n",
       "             ('sister', 9),\n",
       "             ('under', 9),\n",
       "             ('close', 9),\n",
       "             ('flick', 9),\n",
       "             ('cool', 9),\n",
       "             ('standard', 9),\n",
       "             ('earth', 9),\n",
       "             ('continu', 9),\n",
       "             ('visual', 9),\n",
       "             ('neither', 9),\n",
       "             ('english', 9),\n",
       "             ('romant', 9),\n",
       "             ('washington', 9),\n",
       "             ('alreadi', 9),\n",
       "             ('realiz', 9),\n",
       "             ('jame', 9),\n",
       "             ('post', 9),\n",
       "             ('captur', 9),\n",
       "             ('pace', 9),\n",
       "             ('hold', 9),\n",
       "             ('weak', 9),\n",
       "             ('recent', 9),\n",
       "             ('law', 9),\n",
       "             ('reach', 9),\n",
       "             ('whether', 9),\n",
       "             ('societi', 9),\n",
       "             ('remain', 9),\n",
       "             ('compar', 9),\n",
       "             ('across', 9),\n",
       "             ('girlfriend', 9),\n",
       "             ('gay', 9),\n",
       "             ('five', 9),\n",
       "             ('overal', 9),\n",
       "             ('drag', 9),\n",
       "             ('social', 9),\n",
       "             ('talent', 9),\n",
       "             ('dialog', 9),\n",
       "             ('cheesi', 9),\n",
       "             ('break', 9),\n",
       "             ('3', 9),\n",
       "             ('popular', 9),\n",
       "             ('level', 9),\n",
       "             ('\")', 9),\n",
       "             ('detail', 9),\n",
       "             ('local', 9),\n",
       "             ('sexual', 9),\n",
       "             ('..', 9),\n",
       "             ('ago', 9),\n",
       "             ('despit', 9),\n",
       "             ('award', 9),\n",
       "             ('negat', 9),\n",
       "             ('women', 9),\n",
       "             ('ridicul', 9),\n",
       "             ('normal', 9),\n",
       "             ('terri', 9),\n",
       "             ('past', 9),\n",
       "             ('tarzan', 9),\n",
       "             ('visit', 9),\n",
       "             ('support', 9),\n",
       "             ('caus', 9),\n",
       "             ('contest', 9),\n",
       "             ('sequenc', 9),\n",
       "             ('matrix', 9),\n",
       "             ('bland', 9),\n",
       "             ('tom', 9),\n",
       "             ('captain', 9),\n",
       "             ('pain', 9),\n",
       "             ('janeway', 9),\n",
       "             ('rent', 8),\n",
       "             ('promis', 8),\n",
       "             ('channel', 8),\n",
       "             ('cri', 8),\n",
       "             ('bobbi', 8),\n",
       "             ('introduc', 8),\n",
       "             ('attack', 8),\n",
       "             ('attempt', 8),\n",
       "             ('angel', 8),\n",
       "             ('strike', 8),\n",
       "             ('attent', 8),\n",
       "             ('cheap', 8),\n",
       "             ('intent', 8),\n",
       "             ('fashion', 8),\n",
       "             ('romanc', 8),\n",
       "             ('greatest', 8),\n",
       "             ('train', 8),\n",
       "             ('non', 8),\n",
       "             ('rule', 8),\n",
       "             ('earli', 8),\n",
       "             ('novel', 8),\n",
       "             ('honest', 8),\n",
       "             ('admir', 8),\n",
       "             ('cinematographi', 8),\n",
       "             ('imagin', 8),\n",
       "             ('difficult', 8),\n",
       "             ('knew', 8),\n",
       "             ('state', 8),\n",
       "             ('redempt', 8),\n",
       "             ('water', 8),\n",
       "             ('soon', 8),\n",
       "             ('murder', 8),\n",
       "             ('relationship', 8),\n",
       "             ('edg', 8),\n",
       "             ('seat', 8),\n",
       "             ('similar', 8),\n",
       "             ('posit', 8),\n",
       "             ('odd', 8),\n",
       "             ('side', 8),\n",
       "             ('otherwis', 8),\n",
       "             ('number', 8),\n",
       "             ('heard', 8),\n",
       "             ('pure', 8),\n",
       "             ('gore', 8),\n",
       "             ('attract', 8),\n",
       "             ('happi', 8),\n",
       "             ('bank', 8),\n",
       "             ('prove', 8),\n",
       "             ('herself', 8),\n",
       "             ('twist', 8),\n",
       "             ('learn', 8),\n",
       "             ('insid', 8),\n",
       "             ('experi', 8),\n",
       "             ('plain', 8),\n",
       "             ('date', 8),\n",
       "             ('critic', 8),\n",
       "             ('genuin', 8),\n",
       "             ('adapt', 8),\n",
       "             ('hear', 8),\n",
       "             ('hate', 8),\n",
       "             ('ladi', 8),\n",
       "             ('return', 8),\n",
       "             ('technolog', 8),\n",
       "             ('bug', 8),\n",
       "             ('motion', 8),\n",
       "             ('suggest', 8),\n",
       "             ('game', 8),\n",
       "             ('street', 8),\n",
       "             ('voyag', 8),\n",
       "             ('dub', 8),\n",
       "             ('alon', 8),\n",
       "             ('premis', 8),\n",
       "             ('oscar', 8),\n",
       "             ('robot', 8),\n",
       "             ('arthur', 8),\n",
       "             ('morti', 8),\n",
       "             ('humor', 7),\n",
       "             ('5', 7),\n",
       "             ('rock', 7),\n",
       "             ('gave', 7),\n",
       "             ('middl', 7),\n",
       "             ('cannot', 7),\n",
       "             ('straight', 7),\n",
       "             ('handl', 7),\n",
       "             ('serpent', 7),\n",
       "             ('mix', 7),\n",
       "             ('colour', 7),\n",
       "             ('photographi', 7),\n",
       "             ('seven', 7),\n",
       "             ('project', 7),\n",
       "             ('easili', 7),\n",
       "             ('notic', 7),\n",
       "             ('dark', 7),\n",
       "             ('....', 7),\n",
       "             ('depress', 7),\n",
       "             ('focus', 7),\n",
       "             ('flat', 7),\n",
       "             ('ozu', 7),\n",
       "             ('univers', 7),\n",
       "             ('concern', 7),\n",
       "             ('connect', 7),\n",
       "             ('sing', 7),\n",
       "             ('haven', 7),\n",
       "             ('disturb', 7),\n",
       "             ('hasn', 7),\n",
       "             ('agre', 7),\n",
       "             ('jone', 7),\n",
       "             ('richard', 7),\n",
       "             ('attitud', 7),\n",
       "             ('decent', 7),\n",
       "             ('huge', 7),\n",
       "             ('spi', 7),\n",
       "             ('imag', 7),\n",
       "             ('convict', 7),\n",
       "             ('danc', 7),\n",
       "             ('brother', 7),\n",
       "             ('team', 7),\n",
       "             ('hell', 7),\n",
       "             ('longer', 7),\n",
       "             ('creativ', 7),\n",
       "             ('pick', 7),\n",
       "             ('ball', 7),\n",
       "             ('babi', 7),\n",
       "             ('militari', 7),\n",
       "             ('background', 7),\n",
       "             ('imdb', 7),\n",
       "             ('type', 7),\n",
       "             ('fantast', 7),\n",
       "             ('suffer', 7),\n",
       "             ('okay', 7),\n",
       "             ('minor', 7),\n",
       "             ('depart', 7),\n",
       "             ('yourself', 7),\n",
       "             ('combin', 7),\n",
       "             ('mysteri', 7),\n",
       "             ('sometim', 7),\n",
       "             ('note', 7),\n",
       "             ('redeem', 7),\n",
       "             ('compani', 7),\n",
       "             ('uniqu', 7),\n",
       "             ('academi', 7),\n",
       "             ('nomin', 7),\n",
       "             ('15', 7),\n",
       "             ('exact', 7),\n",
       "             ('explor', 7),\n",
       "             ('period', 7),\n",
       "             ('trash', 7),\n",
       "             ('british', 7),\n",
       "             ('simpl', 7),\n",
       "             ('search', 7),\n",
       "             ('husband', 7),\n",
       "             ('billi', 7),\n",
       "             ('appreci', 7),\n",
       "             ('bodi', 7),\n",
       "             ('!)', 7),\n",
       "             ('stun', 7),\n",
       "             ('jr', 7),\n",
       "             ('struggl', 7),\n",
       "             ('fire', 7),\n",
       "             ('queen', 7),\n",
       "             ('dress', 7),\n",
       "             ('photograph', 7),\n",
       "             ('fear', 7),\n",
       "             ('homeless', 7),\n",
       "             ('requir', 7),\n",
       "             ('crocodil', 7),\n",
       "             ('hunter', 7),\n",
       "             ('bolt', 7),\n",
       "             ('pia', 7),\n",
       "             ('guillotin', 7),\n",
       "             ('borg', 7),\n",
       "             ('maker', 6),\n",
       "             ('ahead', 6),\n",
       "             ('career', 6),\n",
       "             ('air', 6),\n",
       "             ('free', 6),\n",
       "             ('4', 6),\n",
       "             ('scare', 6),\n",
       "             ('remind', 6),\n",
       "             ('chase', 6),\n",
       "             ('trip', 6),\n",
       "             ('tone', 6),\n",
       "             ('lesson', 6),\n",
       "             ('forward', 6),\n",
       "             ('ugli', 6),\n",
       "             ('conclus', 6),\n",
       "             ('inde', 6),\n",
       "             ('match', 6),\n",
       "             ('blue', 6),\n",
       "             ('listen', 6),\n",
       "             ('explain', 6),\n",
       "             ('cold', 6),\n",
       "             ('aspect', 6),\n",
       "             ('mediocr', 6),\n",
       "             ('amateurish', 6),\n",
       "             ('ben', 6),\n",
       "             ('.....', 6),\n",
       "             ('consist', 6),\n",
       "             ('previous', 6),\n",
       "             ('copi', 6),\n",
       "             ('asid', 6),\n",
       "             ('york', 6),\n",
       "             ('giant', 6),\n",
       "             ('nation', 6),\n",
       "             ('accord', 6),\n",
       "             ('conflict', 6),\n",
       "             ('resembl', 6),\n",
       "             ('intrigu', 6),\n",
       "             ('!!', 6),\n",
       "             ('suck', 6),\n",
       "             ('master', 6),\n",
       "             ('convinc', 6),\n",
       "             ('hsiao', 6),\n",
       "             ('hsien', 6),\n",
       "             ('held', 6),\n",
       "             ('reaction', 6),\n",
       "             ('atmospher', 6),\n",
       "             ('trailer', 6),\n",
       "             ('confront', 6),\n",
       "             ('polit', 6),\n",
       "             ('amount', 6),\n",
       "             ('impress', 6),\n",
       "             ('masterpiec', 6),\n",
       "             ('freedom', 6),\n",
       "             ('kevin', 6),\n",
       "             ('win', 6),\n",
       "             ('realiti', 6),\n",
       "             ('secret', 6),\n",
       "             ('polic', 6),\n",
       "             ('plus', 6),\n",
       "             ('tear', 6),\n",
       "             ('quot', 6),\n",
       "             ('above', 6),\n",
       "             ('whatsoev', 6),\n",
       "             ('realist', 6),\n",
       "             ('river', 6),\n",
       "             ('arriv', 6),\n",
       "             ('tension', 6),\n",
       "             ('?)', 6),\n",
       "             ('encount', 6),\n",
       "             ('member', 6),\n",
       "             ('barbara', 6),\n",
       "             ('witch', 6),\n",
       "             ('hurt', 6),\n",
       "             ('extra', 6),\n",
       "             ('marriag', 6),\n",
       "             ('william', 6),\n",
       "             ('check', 6),\n",
       "             ('sudden', 6),\n",
       "             ('epic', 6),\n",
       "             ('fulli', 6),\n",
       "             ('draw', 6),\n",
       "             ('beyond', 6),\n",
       "             ('swedish', 6),\n",
       "             ('crimin', 6),\n",
       "             ('intern', 6),\n",
       "             ('unless', 6),\n",
       "             ('becam', 6),\n",
       "             ('skill', 6),\n",
       "             ('aim', 6),\n",
       "             ('stick', 6),\n",
       "             ('claim', 6),\n",
       "             ...])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocabulary()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2706),\n",
       " ('a', 1361),\n",
       " ('and', 1349),\n",
       " ('of', 1205),\n",
       " ('to', 1115),\n",
       " ('is', 815),\n",
       " ('it', 786),\n",
       " ('in', 719),\n",
       " ('i', 690),\n",
       " ('this', 594),\n",
       " ('that', 581),\n",
       " ('s', 541),\n",
       " ('movi', 459),\n",
       " ('film', 400),\n",
       " ('as', 377),\n",
       " ('but', 358),\n",
       " ('with', 357),\n",
       " ('for', 315),\n",
       " ('was', 305),\n",
       " ('t', 295)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn into a list of tuples and get the first 20 items\n",
    "list(vocab.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that many of the most common words in the reviews are what we would consider stopwords: determiners like \"the,\" \"a\"; prepositions like \"of,\" \"to\"; etc. We will probably want to filter these out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text through a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./media/bag_of_words.png\" width=\"600\">\n",
    "\n",
    "<center> Source: <ahref=\"https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8\">Representing text in NLP</a> </center>\n",
    "\n",
    "\n",
    "Now that we have our vocabulary, we can vectorize our documents. The value of our features will be the most simple vectorization of a document there is - the word counts.\n",
    "\n",
    "By doing this, each column value is the number of times that word of the vocabulary appeared in the document. This is what is called a **Bag of Words** (BoW) representation.\n",
    "\n",
    "Note that this type of vectorization of the document loses all of its syntactic information. That is, you could shuffle the words in the document and get the same vector (that's why it's called a bag of words). Of course, since we are trying to understand if a movie review is positive or negative, one could argue that what really matters as features is what kind of words appear in the documents and not their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize():\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in build_vocabulary()])\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this better if we use a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(vocabulary):\n",
    "    return pd.DataFrame(vectorize(), columns=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>in</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  5740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   the   a  and  of  to  is  it  in   i  this  ...  championship  ...\"  \\\n",
       "0   11  94    7   4   7  16   5  11  76     3  ...             0     0   \n",
       "1    0   6    0   0   1   3   2   0  11     1  ...             0     0   \n",
       "2    8  48    6   3   1   6   3   5  35     1  ...             0     0   \n",
       "3    8  36    3   3   3   5   6   4  45     2  ...             0     0   \n",
       "4   21  70    9   4  16  13   6  15  73     1  ...             0     0   \n",
       "\n",
       "   endear  cortney  incid  erupt  semblanc  miser  shoe  mail  \n",
       "0       0        0      0      0         0      0     0     0  \n",
       "1       0        0      0      0         0      0     0     0  \n",
       "2       0        0      0      0         0      0     0     0  \n",
       "3       0        0      0      0         0      0     0     0  \n",
       "4       0        0      0      0         0      0     0     0  \n",
       "\n",
       "[5 rows x 5740 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_df(vocab).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned stopwords briefly in the last learning notebook, but now we will go a bit more in-depth. \n",
    "\n",
    "We're looking for the most meaningful features in our vocabulary to tell us in what category our document falls into. Text is filled with words that are unimportant to the meaning of a particular sentence like \"the\" or \"and\". This is in contrast with words like \"love\" or \"hate\" that have a very clear semantic meaning. The former example of words are called **stopwords** - words that _usually_ don't introduce any meaning to a piece of text and are often just in the document for syntactic reasons.\n",
    "\n",
    "It's important to emphasize that we used \"usually\" in our previous statement. You should be aware that sometimes stopwords can be useful features, especially when we use more than just unigrams as features (ex.: bigrams, trigrams, ...), and word order and word combination starts to be relevant.\n",
    " \n",
    "You can easily find lists of stopwords for several languages on the internet. You can find one for English in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [line.strip(\"\\n\") for line in open(\"./data/english_stopwords.txt\", \"r\")]\n",
    "\n",
    "stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our `build_vocabulary()` and `vectorize()` functions and remove these words from the text. This way we will reduce our vocabulary - and thus our feature space - making our representations more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_without_stopwords():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in stopwords]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "vocab_without_stopwords = build_vocabulary_without_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the new vocabulary (should be smaller than before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5612"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movi', 459),\n",
       " ('film', 400),\n",
       " ('one', 238),\n",
       " ('like', 204),\n",
       " ('time', 124),\n",
       " ('get', 118),\n",
       " ('watch', 110),\n",
       " ('make', 106),\n",
       " ('even', 105),\n",
       " ('see', 105),\n",
       " ('good', 99),\n",
       " ('stori', 99),\n",
       " ('charact', 98),\n",
       " ('end', 97),\n",
       " ('scene', 96),\n",
       " ('would', 94),\n",
       " ('well', 92),\n",
       " ('much', 92),\n",
       " ('peopl', 92),\n",
       " ('love', 86)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn into a list of tuples and get the first 20 items\n",
    "list(vocab_without_stopwords.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  5612 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movi  film  one  like  time  get  watch  make  even  see  ...  \\\n",
       "0     3     5    0     0     2    1      1     3     0    0  ...   \n",
       "1     1     0    0     0     0    0      0     0     0    0  ...   \n",
       "2     6     0    2     1     0    0      0     0     1    0  ...   \n",
       "3     4     0    1     1     1    0      0     0     1    2  ...   \n",
       "4     1     0    1     0     1    1      1     0     0    4  ...   \n",
       "\n",
       "   championship  ...\"  endear  cortney  incid  erupt  semblanc  miser  shoe  \\\n",
       "0             0     0       0        0      0      0         0      0     0   \n",
       "1             0     0       0        0      0      0         0      0     0   \n",
       "2             0     0       0        0      0      0         0      0     0   \n",
       "3             0     0       0        0      0      0         0      0     0   \n",
       "4             0     0       0        0      0      0         0      0     0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 5612 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize():\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocab_without_stopwords])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "BoW = build_df(vocab_without_stopwords)\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we could do is to normalize our counts. As you can see, different documents have different number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    859\n",
       "1    106\n",
       "2    513\n",
       "3    502\n",
       "4    935\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can introduce bias in our features, so we should normalize each document by its number of words. This way, instead of having word counts as features of our model, we will have **term frequencies**. This way, the features in any document of the dataset sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = BoW.div(BoW.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  5612 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         movi      film       one      like      time  get  watch      make  \\\n",
       "133  0.000000  0.000000  0.002020  0.002020  0.000000  0.0    0.0  0.000000   \n",
       "39   0.000000  0.004211  0.000000  0.000000  0.002105  0.0    0.0  0.002105   \n",
       "64   0.000682  0.000682  0.000682  0.001363  0.000682  0.0    0.0  0.000000   \n",
       "\n",
       "         even       see  ...  championship  ...\"  endear  cortney  incid  \\\n",
       "133  0.000000  0.000000  ...           0.0   0.0     0.0      0.0    0.0   \n",
       "39   0.000000  0.000000  ...           0.0   0.0     0.0      0.0    0.0   \n",
       "64   0.000682  0.002045  ...           0.0   0.0     0.0      0.0    0.0   \n",
       "\n",
       "     erupt  semblanc  miser  shoe  mail  \n",
       "133    0.0       0.0    0.0   0.0   0.0  \n",
       "39     0.0       0.0    0.0   0.0   0.0  \n",
       "64     0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 5612 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kicking it up a notch with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear by now that not all words have the same importance to find out in what category a document falls into.\n",
    "\n",
    "In our dataset, if we want to classify a review as positive, for instance, the word \"*good*\" is much more informative than \"*house*\", and we should give it more weight as a feature.\n",
    "\n",
    "In general, words that are very common in all classes are less informative, while words that appear particularly connected to a specifc class are more informative. This is usually also the case for rare words, which show up little and may show up only on one particular class.\n",
    "\n",
    "That is the rationale behind **Term Frequency - Inverse Document Frequency (TF-IDF)**:\n",
    "\n",
    "$$ tfidf _{t, d} =(log{(1 + tf_{t,d})})*(log{(1 + \\frac{N}{df_{t}})})  $$\n",
    "\n",
    "where $t$ and $d$ are the term and document for which we are computing a feature, $tf_{t,d}$ is the term frequency of term $t$ in document $d$, $N$ is the total number of documents we have, while $df_{t}$ is the number of documents that contain $t$.\n",
    "\n",
    "We are using the word frequencies from before, but now we are weighting each by the inverse of the number of times they occur in all the documents. The more a word appears in a document and the less it appears in other documents, the higher the TF-IDF of that word in that document.\n",
    "\n",
    "In short, we measure **the term frequency, weighted by its rarity in the entire corpus**.\n",
    "\n",
    "**Note**: TF-IDF can vary in formulation - the idea is always the same, but computation might change slightly. In our case, we are choosing to log-normalize our frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  5612 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movi      film       one      like      time       get     watch  \\\n",
       "0  0.003200  0.005785  0.000000  0.000000  0.002776  0.001352  0.001523   \n",
       "1  0.008618  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.010672  0.000000  0.003342  0.002101  0.000000  0.000000  0.000000   \n",
       "3  0.007284  0.000000  0.001709  0.002147  0.002375  0.000000  0.000000   \n",
       "4  0.000981  0.000000  0.000918  0.000000  0.001276  0.001243  0.001399   \n",
       "\n",
       "       make      even       see  ...  championship  ...\"  endear  cortney  \\\n",
       "0  0.004368  0.000000  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "1  0.000000  0.000000  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "2  0.000000  0.002389  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "3  0.000000  0.002441  0.004029  ...           0.0   0.0     0.0      0.0   \n",
       "4  0.000000  0.000000  0.004325  ...           0.0   0.0     0.0      0.0   \n",
       "\n",
       "   incid  erupt  semblanc  miser  shoe  mail  \n",
       "0    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "1    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "2    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "3    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "4    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 5612 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idf(column):\n",
    "    return np.log(1 + len(column) / sum(column > 0))\n",
    "\n",
    "tf_idf = (np.log(1 + tf)).multiply(BoW.apply(idf))\n",
    "\n",
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing we can do with our feature representations is to check similarities between words. To do this, instead of seeing the vocabulary as the features of a given document, we see each document as a feature of a given term in the vocabulary. By doing this, we get a vectorized representation of a word!\n",
    "\n",
    "A very popular way of computing similarities between vectors is to compute the cosine similarity (the cosine of the angle between the vectors).\n",
    "\n",
    "Let's check the similarity between the word _movi_ and the word _good_ in our Bag of Words representation. These are words that typically show up side by side in movie reviews (at least the positive ones) and as such we would expect a bigger similarity when compared with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46827681]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(BoW['movi'].values.reshape(1,-1), BoW['good'].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute it for _movi_ and _shoe_. We should get a lower similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14554543]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(BoW['movi'].__array__().reshape(1,-1), BoW['shoe'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the same similarity scores but in the tf-idf representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48560461]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf['movi'].__array__().reshape(1,-1), tf_idf['good'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03625309]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf['movi'].__array__().reshape(1,-1), tf_idf['shoe'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The gap between the similarities of these pairs of words increased with our TF-IDF representation. This means that our TF-IDF model is computing better and more meaningful features than our BoW model. This will surely help when we feed these feature matrices to a classifier.\n",
    "\n",
    "**Note**: We are not assessing similarities in general and you should not take this as a general interpretation in the english language (i.e. word X is close to word Y). We're only assessing them in the context of this dataset and its document distribution (i.e. word X has a similar distribution in these docs as word Y), and as such if you try with random words it may not yield what you expect. That's why we speak in relative terms in our comparisons, saying that a set of words has \"higher\" or \"lower\" similarity and not on absolute terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` comes with implementations of both Bag of Words and TF-IDF vectorizers, which you will see in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
